{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[12961]: Class CaptureDelegate is implemented in both /Users/wu/anaconda3/envs/mediapipe/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x145eda840) and /Users/wu/anaconda3/envs/mediapipe/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x124ca4860). One of the two will be used. Which one is undefined.\n",
      "objc[12961]: Class CVWindow is implemented in both /Users/wu/anaconda3/envs/mediapipe/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x145eda890) and /Users/wu/anaconda3/envs/mediapipe/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x121d3ca68). One of the two will be used. Which one is undefined.\n",
      "objc[12961]: Class CVView is implemented in both /Users/wu/anaconda3/envs/mediapipe/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x145eda8b8) and /Users/wu/anaconda3/envs/mediapipe/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x121d3ca90). One of the two will be used. Which one is undefined.\n",
      "objc[12961]: Class CVSlider is implemented in both /Users/wu/anaconda3/envs/mediapipe/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x145eda8e0) and /Users/wu/anaconda3/envs/mediapipe/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x121d3cab8). One of the two will be used. Which one is undefined.\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "W20230419 22:30:09.877797 3714777408 gesture_recognizer_graph.cc:122] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleartion to Xnnpack.\n",
      "I20230419 22:30:09.878563 3714777408 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dual_hand': ['None', 'Open_Palm'], 'hand': None}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from drawing_utils import draw_landmarks\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GestureRecognizer():\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        1. Initialize the gesture recognizer\n",
    "        2. Set parameters\n",
    "        3. Open the camera\n",
    "        \"\"\"\n",
    "        base_options = python.BaseOptions(\n",
    "            model_asset_path='gesture_recognizer.task')\n",
    "        options = vision.GestureRecognizerOptions(base_options=base_options,num_hands=2)\n",
    "        self.recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "        self.direction = None\n",
    "        self.continous_moving_cnt = 0\n",
    "        self.MOVING_THRESHOLD = 50\n",
    "        self.prev_landmarks = None\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        # category: [\"None\", \"Closed_Fist\", \"Open_Palm\", \"Pointing_Up\",\n",
    "        #            \"Thumb_Down\", \"Thumb_Up\", \"Victory\", \"ILoveYou\"]\n",
    "        self.gestures = {'dual_hand': None,'hand':None}\n",
    "        self.freeze = False\n",
    "        \n",
    "    def clear_gesture_cache(self):\n",
    "        self.gestures = {'dual_hand': None,'hand':None}\n",
    "    \n",
    "    \n",
    "    def location_to_direction(self, ):\n",
    "        pass\n",
    "        # for hand_lanmark in self.recog_result['hand_landmarks']:\n",
    "            \n",
    "\n",
    "    def get_command(self):\n",
    "        # gesture is saved by string in one of the following 8 strings: \n",
    "        #       [\"None\", \"Closed_Fist\", \"Open_Palm\", \"Pointing_Up\",\n",
    "        #            \"Thumb_Down\", \"Thumb_Up\", \"Victory\", \"ILoveYou\"]\n",
    "        if self.gestures['dual_hand'] is None:\n",
    "            return None\n",
    "        else:\n",
    "            left_gesture  = self.gestures['dual_hand'][0]\n",
    "            right_gesture = self.gestures['dual_hand'][1]\n",
    "            # left closed_fist, right thumb_up = quick scroll up\n",
    "            # left closed_fist, right thumb_down = quick scroll down\n",
    "            if left_gesture == 'Closed_Fist' and right_gesture == 'Thumb_Up':\n",
    "                return 'quick_scroll_up'\n",
    "            elif left_gesture == 'Closed_Fist' and right_gesture == 'Thumb_Down':\n",
    "                return 'quick_scroll_down'\n",
    "            # left pointing_up, right thumb_up = scroll up\n",
    "            # left pointing_up, right thumb_down = scroll down\n",
    "            if left_gesture == 'Pointing_Up' and right_gesture == 'Thumb_Up':\n",
    "                return 'scroll_up'\n",
    "            elif left_gesture == 'Pointing_Up' and right_gesture == 'Thumb_Down':\n",
    "                return 'scroll_down'\n",
    "            \n",
    "            if left_gesture == 'ILoveYou' and right_gesture == 'Thumb_Up':\n",
    "                return 'slow_scroll_up'\n",
    "            if left_gesture == 'ILoveYou' and right_gesture == 'Thumb_Down':\n",
    "                return 'slow_scroll_down'\n",
    "            \n",
    "            # if both hands open palm = pause\n",
    "            if left_gesture == 'Open_Palm' and right_gesture == 'Open_Palm':\n",
    "                return 'pause'\n",
    "            if left_gesture == 'Thumb_Up' and right_gesture == 'Thumb_Up':\n",
    "                return 'play'\n",
    "            # if both hands pointing up = zoom in\n",
    "            if left_gesture == 'Pointing_Up' and right_gesture == 'Pointing_Up':\n",
    "                return 'zoom_in'\n",
    "            \n",
    "            # if both hands colsed fist = zoom out\n",
    "            if left_gesture == 'Closed_Fist' and right_gesture == 'Closed_Fist':\n",
    "                return 'zoom_out'\n",
    "            \n",
    "            \n",
    "            return None\n",
    "\n",
    "    def run(self,fps=False):\n",
    "        \"\"\"\n",
    "        1. Capture the frame\n",
    "        2. Recognize the gesture\n",
    "        3. Visualize the result\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        ret, frame = self.cap.read()\n",
    "        self.h = frame.shape[0]\n",
    "        self.w = frame.shape[1]\n",
    "        if not ret:\n",
    "            return\n",
    "        self.recog_result = self.recognize(frame)\n",
    "        frame = self.visualize(frame, self.recog_result)\n",
    "        cv2.putText(frame, 'FPS: {:.2f}'.format(1/(time.time()-start_time)), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('Gesture Recognizer', frame)\n",
    "        return\n",
    "\n",
    "    def landmark_cvt_to_numpy(self, landmarks):\n",
    "        \"\"\"\n",
    "        convert landmark to numpy array: \n",
    "            return: np.array[21x3]\n",
    "        \"\"\"\n",
    "        landmarks = self.recog_result.hand_landmarks[0]  # only one hand\n",
    "        # landmarks is a list of 21 landmark_module.NormalizedLandmark\n",
    "        landmark_np = np.array(\n",
    "            [[landmark.x*self.w, landmark.y*self.h] for landmark in landmarks])\n",
    "        return landmark_np\n",
    "\n",
    "    def recognize(self, frame):\n",
    "        # Convert the image to an RGB image\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "        recognition_result = self.recognizer.recognize(mp_image)\n",
    "        return recognition_result\n",
    "\n",
    "    def visualize(self, frame, recog_result):\n",
    "        # Draw the recognized gesture on the frame\n",
    "        if not recog_result.hand_landmarks:\n",
    "            return frame\n",
    "        \n",
    "        for landmark_list in recog_result.hand_landmarks:\n",
    "            draw_landmarks(frame, landmark_list,\n",
    "                    mp.solutions.hands.HAND_CONNECTIONS)\n",
    "        if len(recog_result.handedness) == 2:\n",
    "            # if two hands are detected\n",
    "            if recog_result.handedness[0][0].category_name == 'Left':\n",
    "                left_hand_gesture = recog_result.gestures[recog_result.handedness[0][0].index]\n",
    "                right_hand_gesture = recog_result.gestures[recog_result.handedness[1][0].index]\n",
    "            else:\n",
    "                left_hand_gesture = recog_result.gestures[recog_result.handedness[1][0].index]\n",
    "                right_hand_gesture = recog_result.gestures[recog_result.handedness[0][0].index]\n",
    "            self.gestures['dual_hand'] = [left_hand_gesture[0].category_name, right_hand_gesture[0].category_name]\n",
    "            self.gestures['hand'] = None\n",
    "            cv2.putText(frame, left_hand_gesture[0].category_name, (10, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(frame, right_hand_gesture[0].category_name, (self.w - 240, 100),    \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            self.gestures['dual_hand'] = None\n",
    "            self.gestures['hand'] = recog_result.gestures[0][0].category_name\n",
    "            hand_gesture = recog_result.gestures[0]\n",
    "            cv2.putText(frame, hand_gesture[0].category_name, (10, 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        # for ges in recog_result.gestures:\n",
    "        #     ges_name = ges[0].category_name\n",
    "        #     if ges_name != 'None':\n",
    "        #         self.gestures['category'] = ges_name\n",
    "        #         # put text on cv2 window\n",
    "        #         cv2.putText(frame, ges_name, (10, 30),\n",
    "        #                     cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        # self.update_direction(self.landmark_cvt_to_numpy(\n",
    "        #     recog_result.hand_landmarks))\n",
    "        # self.prev_landmarks = self.landmark_cvt_to_numpy(\n",
    "        #     recog_result.hand_landmarks)\n",
    "        # if self.direction != 'None':\n",
    "        #     self.gestures['direction'] = self.direction\n",
    "        #     cv2.putText(frame, self.direction, (10, 60),\n",
    "        #                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        return frame\n",
    "\n",
    "    def update_direction(self, landmarks):\n",
    "        if self.prev_landmarks is None:\n",
    "            self.direction = None\n",
    "            return\n",
    "        x_diff = landmarks[0, 0] - self.prev_landmarks[0, 0]\n",
    "        y_diff = landmarks[0, 1] - self.prev_landmarks[0, 1]\n",
    "        def diff_to_direction_y(diff):\n",
    "            if abs(diff) < self.MOVING_THRESHOLD:\n",
    "                return 'None'\n",
    "            if diff > 0:\n",
    "                return 'DOWN'\n",
    "            else:\n",
    "                return 'UP'\n",
    "        def diff_to_direction_x(diff):\n",
    "            if abs(diff) < self.MOVING_THRESHOLD:\n",
    "                return 'None'\n",
    "            if diff > 0:\n",
    "                return 'LEFT'\n",
    "            else:\n",
    "                return 'RIGHT'\n",
    "        # print(x_diff, y_diff)\n",
    "        if abs(x_diff) > abs(y_diff):\n",
    "            self.direction = diff_to_direction_x(x_diff)\n",
    "        else:\n",
    "            self.direction = diff_to_direction_y(y_diff)\n",
    "        # self.direction = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gesture_recognizer = GestureRecognizer()\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        gesture_recognizer.run()\n",
    "        if len(gesture_recognizer.recog_result.handedness) == 2:\n",
    "            break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "                    \n",
    "    gestures = gesture_recognizer.gestures\n",
    "    print(gestures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pos = gesture_recognizer.landmark_cvt_to_numpy(gesture_recognizer.recog_result.hand_landmarks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[219.97718811, 750.80403328],\n",
       "       [305.15190125, 740.48186302],\n",
       "       [364.59785461, 704.19359207],\n",
       "       [408.3165741 , 680.85300922],\n",
       "       [458.23547363, 647.46040821],\n",
       "       [372.42923737, 628.68524551],\n",
       "       [426.13773346, 541.33754253],\n",
       "       [458.15994263, 483.92518044],\n",
       "       [488.59367371, 436.69980526],\n",
       "       [304.17446136, 608.96903515],\n",
       "       [332.54302979, 519.59066391],\n",
       "       [347.25887299, 456.03321075],\n",
       "       [365.96096039, 401.74319744],\n",
       "       [231.81951523, 608.88050079],\n",
       "       [236.22642517, 530.30958652],\n",
       "       [242.47371674, 469.75650787],\n",
       "       [247.79548645, 419.35372353],\n",
       "       [162.47097015, 622.76996613],\n",
       "       [147.66023636, 559.11912918],\n",
       "       [139.1547966 , 516.95613384],\n",
       "       [132.97262192, 474.027915  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_pos[:,0] < "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('mediapipe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7027291f33e2d3b42c0f641be97e6a54bd5c4af9d6dc3869f13c5c55b9c81fe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
